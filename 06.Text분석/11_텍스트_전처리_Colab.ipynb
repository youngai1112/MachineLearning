{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11.텍스트_전처리-Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 전처리(Preprocessing)"
      ],
      "metadata": {
        "id": "3fu-gLplk9SC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 토큰화(Tokenization)\n",
        "    - 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업"
      ],
      "metadata": {
        "id": "sTqg-vJlmkCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBgRiYuAnwLn",
        "outputId": "108960d1-3bb4-4f63-e9c2-8588242758f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) 단어 토큰화\n",
        "    - 토큰의 기준을 단어(word)로 하는 경우\n",
        "    - 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주되기도 합니다.\n",
        "    - 입력: Time is an illusion. Lunchtime double so!\n",
        "    ↓ 구두점을 제외시킨 토큰화 작업\n",
        "    - 출력: \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\""
      ],
      "metadata": {
        "id": "YSyFP1rIoUwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "metadata": {
        "id": "oU9zDGbiocay"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 's, n't, '.'도 포함하여 token화 됨\n",
        "# word_tokenize == 함수 <== 소문자\n",
        "# word_tokenize는 Don't를 Do와 n't로 분리\n",
        "# Jone's는 Jone과 's로 분리한 것을 확인\n",
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkwSV6JLpWX6",
        "outputId": "a4cbdfe7-88a4-477f-b314-83ee1476b932"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordPunctTokenizer == Class <== CamelCase\n",
        "# 객체화해서 사용\n",
        "# 구두점을 별도로 분류하는 특징\n",
        "# Don't를 Don과 '와 t로 분리\n",
        "# Jone's를 Jone과 '와 s로 분리한 것을 확인\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "wpt = WordPunctTokenizer()\n",
        "print(wpt.tokenize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X5hs3Hppn_V",
        "outputId": "2b26cddd-1f29-48f9-b050-75881d8eb045"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본적으로 모든 알파벳을 소문자로 바꿈\n",
        "# 마침표나 컴마, 느낌표 등의 구두점을 제거\n",
        "# don't나 jone's와 같은 경우 아포스트로피는 보존\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "print(text_to_word_sequence(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUyLcgyxqarf",
        "outputId": "85aabf34-2765-404a-bdb3-90e5ce5046a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이푼으로 구성된 단어는 하나로 유지\n",
        "# doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tok = TreebankWordTokenizer()\n",
        "print(tok.tokenize(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Owu2g1XqoB4",
        "outputId": "40e2ed6b-9cce-443e-cba8-10f5e64511c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) 문장 토큰화\n",
        "     - 문장 단위로 구분되어지는 리스트를 생성"
      ],
      "metadata": {
        "id": "OWsJ9oKErB2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syoO-v3drcbV",
        "outputId": "9e5a0c66-87cb-408e-a4be-38c8fa2f73d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장을 토큰화 -> 단어 토큰으로 출력\n",
        "# 딥러닝: 감성분석에 이용\n",
        "for sentence in sent_tokenize(text):\n",
        "    print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk1a-fT0sEIN",
        "outputId": "3ce2258a-8c1b-4d99-fa0c-a80ebe594e5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['His', 'barber', 'kept', 'his', 'word', '.']\n",
            "['But', 'keeping', 'such', 'a', 'huge', 'secret', 'to', 'himself', 'was', 'driving', 'him', 'crazy', '.']\n",
            "['Finally', ',', 'the', 'barber', 'went', 'up', 'a', 'mountain', 'and', 'almost', 'to', 'the', 'edge', 'of', 'a', 'cliff', '.']\n",
            "['He', 'dug', 'a', 'hole', 'in', 'the', 'midst', 'of', 'some', 'reeds', '.']\n",
            "['He', 'looked', 'about', ',', 'to', 'make', 'sure', 'no', 'one', 'was', 'near', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6ttbKCysiRQ",
        "outputId": "4e8d70e4-c2f5-4785-828f-7923f0ba5ce5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글 문장 토큰화"
      ],
      "metadata": {
        "id": "jUkzFKT2s67B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KSS (Korean Sentence Splitter) 설치\n",
        "!pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL2gytADtMTM",
        "outputId": "30c1f2ac-6031-4885-eeaa-0427747f670e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (3.3.1.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from kss) (1.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KSS (Korean Sentence Splitter) 설치시 메시지를 보고 싶지 않을 때\n",
        "# !pip install kss > /dev/null"
      ],
      "metadata": {
        "id": "JfzWeLO8tTAv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9sMWDfOuIUW",
        "outputId": "8085bc5c-455d-4ba3-f846-dae5634271c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 55504\n",
            "-rwxr-xr-x 1 root root     1697 Jan  1  2000 anscombe.json\n",
            "-rw-r--r-- 1 root root   301141 Dec 23 14:32 california_housing_test.csv\n",
            "-rw-r--r-- 1 root root  1706430 Dec 23 14:32 california_housing_train.csv\n",
            "-rw-r--r-- 1 root root 18289443 Dec 23 14:32 mnist_test.csv\n",
            "-rw-r--r-- 1 root root 36523880 Dec 23 14:32 mnist_train_small.csv\n",
            "-rwxr-xr-x 1 root root      930 Jan  1  2000 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- cmd (command) : 명령어\n",
        "- shell\n",
        "- !ls -l\n",
        "- 프로그램 설치: pip install module\n",
        "- 한글 폰트 설치: apt-get\n",
        "- 파일 업로드: 시간이 오래걸림\n",
        "- 파일 --> Google Drive --> Colab에서 사용하게 조처 --> 빠른 시간 mount\n",
        "\n",
        "- Linux(unix) 명령어를 알아둘 필요가 있다.\n"
      ],
      "metadata": {
        "id": "wyut_LJxuIP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqOccYswuINn",
        "outputId": "eaed17dd-dec1-47b9-aa73-26306a81f8a4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Korean Sentence Splitter]: Initializing Pynori...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한국어 토큰화의 어려움\n",
        "    - 영어: 줄임말 예외처리 외, 띄어쓰기를 기준으로 토큰화 수행 가능\n",
        "    - 한국어: 어절 토큰화X -> **형태소**\n",
        "        - 어절 토큰화 != 단어 토큰화\n",
        "        - 교착어: 명사/동사 + 조사/어미\n",
        "        - 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용\n",
        "            - 그 자체로 단어\n",
        "            - 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사\n",
        "\n",
        "        - 의존 형태소 : 다른 형태소와 결합하여 사용\n",
        "            - 접사, 어미, 조사, 어간\n",
        "    - 국어 문법 참고: https://m.blog.naver.com/zzangdol57/221546199789\n",
        "    - https://www.korean.go.kr/front/onlineQna/onlineQnaView.do?mn_id=216&qna_seq=124353\n",
        "    - https://m.blog.naver.com/st004329/220860481805"
      ],
      "metadata": {
        "id": "pm6tO0SVuILm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 품사 태깅\n",
        "    - 단어의 의미를 제대로 파악하기 위해서 해당 단어가 어떤 품사로 쓰였는지 확인해야\n",
        "    - 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지 구분하는 것\n",
        "    - 참고: https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/ "
      ],
      "metadata": {
        "id": "pTxdSCbKuII3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPdmBnFUuIGn",
        "outputId": "769219b3-9fa6-4889-d72d-81b77976e543"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsbizVd_1gZX",
        "outputId": "23a5fac8-1a98-480b-d55f-90778fae3065"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK에서는 Penn Treebank POS Tags라는 기준을 사용하여 품사를 태깅\n",
        "from nltk.tag import pos_tag\n",
        "pos_tag(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB_4TOahuIEP",
        "outputId": "b941b661-ff63-49b1-bf33-151b3919fd10"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " (' ', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('m', 'NN'),\n",
              " (' ', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('c', 'JJ'),\n",
              " ('t', 'NN'),\n",
              " ('i', 'NN'),\n",
              " ('v', 'VBP'),\n",
              " ('e', 'NN'),\n",
              " ('l', 'NN'),\n",
              " ('y', 'NN'),\n",
              " (' ', 'NNP'),\n",
              " ('l', 'NN'),\n",
              " ('o', 'NN'),\n",
              " ('o', 'IN'),\n",
              " ('k', 'NN'),\n",
              " ('i', 'NN'),\n",
              " ('n', 'VBP'),\n",
              " ('g', 'NN'),\n",
              " (' ', 'NNP'),\n",
              " ('f', 'NN'),\n",
              " ('o', 'NN'),\n",
              " ('r', 'NN'),\n",
              " (' ', 'NNP'),\n",
              " ('P', 'NNP'),\n",
              " ('h', 'NN'),\n",
              " ('.', '.'),\n",
              " ('D', 'NNP'),\n",
              " ('.', '.'),\n",
              " (' ', 'VB'),\n",
              " ('s', 'JJ'),\n",
              " ('t', 'NN'),\n",
              " ('u', 'JJ'),\n",
              " ('d', 'NN'),\n",
              " ('e', 'NN'),\n",
              " ('n', 'JJ'),\n",
              " ('t', 'NN'),\n",
              " ('s', 'NN'),\n",
              " ('.', '.'),\n",
              " (' ', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('n', 'JJ'),\n",
              " ('d', 'NN'),\n",
              " (' ', 'NNP'),\n",
              " ('y', 'NN'),\n",
              " ('o', 'NN'),\n",
              " ('u', 'JJ'),\n",
              " (' ', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('r', 'NN'),\n",
              " ('e', 'NN'),\n",
              " (' ', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " (' ', 'JJ'),\n",
              " ('P', 'NNP'),\n",
              " ('h', 'NN'),\n",
              " ('.', '.'),\n",
              " ('D', 'NNP'),\n",
              " ('.', '.'),\n",
              " (' ', 'VB'),\n",
              " ('s', 'JJ'),\n",
              " ('t', 'NN'),\n",
              " ('u', 'JJ'),\n",
              " ('d', 'NN'),\n",
              " ('e', 'NN'),\n",
              " ('n', 'JJ'),\n",
              " ('t', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 펜트리뱅트 캐드세트에서 사용하는 품사의 예\n",
        "\n",
        "NNP: 단수 고유명사\n",
        "\n",
        "VB: 동사\n",
        "\n",
        "VBP: 동사 현재형\n",
        "\n",
        "TO: to 전치사\n",
        "\n",
        "NN: 명사(단수형 혹은 집합형)\n",
        "\n",
        "DT: 관형사\n",
        "\n",
        "PRP: 인칭 대명사\n",
        "\n",
        "RB: 부사\n",
        "\n",
        "그 밖에도, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사"
      ],
      "metadata": {
        "id": "vCd4k3CQuIB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글 (KoNLPy:코엔엘파이)"
      ],
      "metadata": {
        "id": "QUVtpnOluH0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoNLPy 설치 - PC에서 Okt를 주로 사용\n",
        "# 고속 처리: Mecab(단, Linux에는 없음, PC사용 불가)\n",
        "!pip install KoNLPy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgqj7fcT3a_s",
        "outputId": "670514f7-a63c-4ee8-af3b-16f104a8f192"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: KoNLPy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from KoNLPy) (1.3.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from KoNLPy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from KoNLPy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->KoNLPy) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Okt(Open Korean Text)"
      ],
      "metadata": {
        "id": "SycqWie_4ilQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소 분석 : 벡터화 관련\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "text=\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "okt.morphs(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xShAtRGd3h7s",
        "outputId": "2b94f372-8b35-4f8f-8c81-129cecba49fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stem 파라미터를 True로 입력하면 원형이 나온다.\n",
        "# 벡터화 관련\n",
        "okt.morphs(text, stem=True)     # 용언(동사, 형용사)은 어간을 추출함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAtQcbQA4fGE",
        "outputId": "a8d75ded-c00d-4b0e-bbad-27a1c98cde1c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가보다']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 품사 부착\n",
        "okt.pos(text) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGxtI7lt4vzV",
        "outputId": "5930258a-1393-46f2-af2a-ad0930774fa2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('열심히', 'Adverb'),\n",
              " ('코딩', 'Noun'),\n",
              " ('한', 'Josa'),\n",
              " ('당신', 'Noun'),\n",
              " (',', 'Punctuation'),\n",
              " ('연휴', 'Noun'),\n",
              " ('에는', 'Josa'),\n",
              " ('여행', 'Noun'),\n",
              " ('을', 'Josa'),\n",
              " ('가봐요', 'Verb')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 명사 추출\n",
        "# 단어 군집화 == Word Crowd (명사만 추출)\n",
        "okt.nouns(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6nFj6PS5HsV",
        "outputId": "6c210658-fcd1-40ff-8cd4-62cf822da272"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 부착되는 품사 태그의 기호와 의미 알아보기\n",
        "okt.tagset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b4lLiNI5VO0",
        "outputId": "36531763-bc97-40ea-be30-b884e9e730ef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Adjective': '형용사',\n",
              " 'Adverb': '부사',\n",
              " 'Alpha': '알파벳',\n",
              " 'Conjunction': '접속사',\n",
              " 'Determiner': '관형사',\n",
              " 'Eomi': '어미',\n",
              " 'Exclamation': '감탄사',\n",
              " 'Foreign': '외국어, 한자 및 기타기호',\n",
              " 'Hashtag': '트위터 해쉬태그',\n",
              " 'Josa': '조사',\n",
              " 'KoreanParticle': '(ex: ㅋㅋ)',\n",
              " 'Noun': '명사',\n",
              " 'Number': '숫자',\n",
              " 'PreEomi': '선어말어미',\n",
              " 'Punctuation': '구두점',\n",
              " 'ScreenName': '트위터 아이디',\n",
              " 'Suffix': '접미사',\n",
              " 'Unknown': '미등록어',\n",
              " 'Verb': '동사'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 꼬꼬마"
      ],
      "metadata": {
        "id": "JUFCYUdx5k7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "kkma.morphs(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy11I7my57c2",
        "outputId": "3667a58f-0fdf-4f61-b911-2eb83e22afc0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.pos(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAykGez56EZp",
        "outputId": "72d33374-b507-467a-f36b-7cd6d9611e34"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('열심히', 'MAG'),\n",
              " ('코딩', 'NNG'),\n",
              " ('하', 'XSV'),\n",
              " ('ㄴ', 'ETD'),\n",
              " ('당신', 'NP'),\n",
              " (',', 'SP'),\n",
              " ('연휴', 'NNG'),\n",
              " ('에', 'JKM'),\n",
              " ('는', 'JX'),\n",
              " ('여행', 'NNG'),\n",
              " ('을', 'JKO'),\n",
              " ('가보', 'VV'),\n",
              " ('아요', 'EFN')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma.nouns(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drA_9ogi6Vn7",
        "outputId": "d47bc8c8-2317-4b16-b1a7-5fc754cae0a7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 부착되는 품사 태그의 기호와 의미 알아보기\n",
        "kkma.tagset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYRbcaVu6a62",
        "outputId": "40cc37ba-098e-468d-f3b0-528d9c81ccfd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'EC': '연결 어미',\n",
              " 'ECD': '의존적 연결 어미',\n",
              " 'ECE': '대등 연결 어미',\n",
              " 'ECS': '보조적 연결 어미',\n",
              " 'EF': '종결 어미',\n",
              " 'EFA': '청유형 종결 어미',\n",
              " 'EFI': '감탄형 종결 어미',\n",
              " 'EFN': '평서형 종결 어미',\n",
              " 'EFO': '명령형 종결 어미',\n",
              " 'EFQ': '의문형 종결 어미',\n",
              " 'EFR': '존칭형 종결 어미',\n",
              " 'EP': '선어말 어미',\n",
              " 'EPH': '존칭 선어말 어미',\n",
              " 'EPP': '공손 선어말 어미',\n",
              " 'EPT': '시제 선어말 어미',\n",
              " 'ET': '전성 어미',\n",
              " 'ETD': '관형형 전성 어미',\n",
              " 'ETN': '명사형 전성 어미',\n",
              " 'IC': '감탄사',\n",
              " 'JC': '접속 조사',\n",
              " 'JK': '조사',\n",
              " 'JKC': '보격 조사',\n",
              " 'JKG': '관형격 조사',\n",
              " 'JKI': '호격 조사',\n",
              " 'JKM': '부사격 조사',\n",
              " 'JKO': '목적격 조사',\n",
              " 'JKQ': '인용격 조사',\n",
              " 'JKS': '주격 조사',\n",
              " 'JX': '보조사',\n",
              " 'MA': '부사',\n",
              " 'MAC': '접속 부사',\n",
              " 'MAG': '일반 부사',\n",
              " 'MD': '관형사',\n",
              " 'MDN': '수 관형사',\n",
              " 'MDT': '일반 관형사',\n",
              " 'NN': '명사',\n",
              " 'NNB': '일반 의존 명사',\n",
              " 'NNG': '보통명사',\n",
              " 'NNM': '단위 의존 명사',\n",
              " 'NNP': '고유명사',\n",
              " 'NP': '대명사',\n",
              " 'NR': '수사',\n",
              " 'OH': '한자',\n",
              " 'OL': '외국어',\n",
              " 'ON': '숫자',\n",
              " 'SE': '줄임표',\n",
              " 'SF': '마침표, 물음표, 느낌표',\n",
              " 'SO': '붙임표(물결,숨김,빠짐)',\n",
              " 'SP': '쉼표,가운뎃점,콜론,빗금',\n",
              " 'SS': '따옴표,괄호표,줄표',\n",
              " 'SW': '기타기호 (논리수학기호,화폐기호)',\n",
              " 'UN': '명사추정범주',\n",
              " 'VA': '형용사',\n",
              " 'VC': '지정사',\n",
              " 'VCN': \"부정 지정사, 형용사 '아니다'\",\n",
              " 'VCP': \"긍정 지정사, 서술격 조사 '이다'\",\n",
              " 'VV': '동사',\n",
              " 'VX': '보조 용언',\n",
              " 'VXA': '보조 형용사',\n",
              " 'VXV': '보조 동사',\n",
              " 'XP': '접두사',\n",
              " 'XPN': '체언 접두사',\n",
              " 'XPV': '용언 접두사',\n",
              " 'XR': '어근',\n",
              " 'XSA': '형용사 파생 접미사',\n",
              " 'XSN': '명사파생 접미사',\n",
              " 'XSV': '동사 파생 접미사'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 정제(Cleaning)와 정규화(Normalization)\n",
        "    - 코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(tokenization)\n",
        "    - 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning) 및 정규화(normalization)\n",
        "\n",
        "    - 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
        "    - 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다."
      ],
      "metadata": {
        "id": "MRT0n2uZ6it4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1. 규칙에 기반한 표기가 다른 단어들의 통합\n",
        "    - 예시: USA == US\n",
        "- 2. 대,소문자 통합\n",
        "    - 예시: 검색엔진에서 a Ferrari car == ferrari\n",
        "    - 예시: bush != Bush(사람이름)\n",
        "- 3. 불필요한 단어의 제거\n",
        "    - 등장빈도가 적은 단어\n",
        "    - 길이가 짧은 단어"
      ],
      "metadata": {
        "id": "eHKkndXV6-5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\""
      ],
      "metadata": {
        "id": "CiILassW_aYa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "# 이 방법은 거의 사용하지 않는다.\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "shortword.sub('', text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WNBI7V-F_iZs",
        "outputId": "5cad238a-defe-4d9b-e002-8baf7f34bf0f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' was wondering anyone out there could enlighten this car.'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화한 후 길이가 2보다 큰 단어만 발췌\n",
        "# 리스트 컴프리헨션을 이용하여 불용어 제거하기\n",
        "# 실무에서 자주 사용하는 방법\n",
        "[word for word in word_tokenize(text) if len(word) > 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuWivzuY_6j1",
        "outputId": "e72d730c-b191-40f3-d0fe-fa54f1b38ca6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['was',\n",
              " 'wondering',\n",
              " 'anyone',\n",
              " 'out',\n",
              " 'there',\n",
              " 'could',\n",
              " 'enlighten',\n",
              " 'this',\n",
              " 'car']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = ' '.join([word for word in word_tokenize(text) if len(word) > 2])\n",
        "clean_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o5CwQf2fAL7c",
        "outputId": "13e0a219-71f1-45d6-b475-59e0feee2936"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'was wondering anyone out there could enlighten this car'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 어간 추출(Stemming) 및 표제어 추출(Lemmatization)"
      ],
      "metadata": {
        "id": "EC-lMUL9ASaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) 표제어 추출\n",
        "    - 표제어란?: 기본 사전형 단어\n",
        "    - 예시: am, are, is => 표제어는 be\n",
        "    - 표제어 추출 방법: 단어의 형태학적 파싱을 먼저 진행\n",
        "        - 어간: 단어의 의미를 담는 부분\n",
        "        - 접사: 단어에 추가적 의미 부여"
      ],
      "metadata": {
        "id": "3FIKm5VmBQpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdshCprtB4n1",
        "outputId": "6255224c-8756-44d8-f846-6a65cb34e159"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "gTWAr8nxCEZx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([lemma.lemmatize(word) for word in words]) # map을 사용해도 된다.\n",
        "                                                 # print를 사용X => 세로로 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXxjxDw9CY6W",
        "outputId": "3ec88620-2aa4-430c-a686-32ef2e9fc1ef"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemma.lemmatize(단어, 품사) => 원형을 추출하기 위해 설정\n",
        "lemma.lemmatize('lives','v'), lemma.lemmatize('dies','v'), lemma.lemmatize('watched','v'), lemma.lemmatize('has','v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JzSsI8BCzJ5",
        "outputId": "3f0202a4-1656-4356-aca0-0d6eb09b79d0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('live', 'die', 'watch', 'have')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 스펠링이 같으나 의미가 다른 경우\n",
        "lemma.lemmatize('flies','v'), lemma.lemmatize('flies','n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSDj_d8oDdgf",
        "outputId": "cea1c97a-c51c-4025-c5fe-4cfc721878a5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fly', 'fly')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize('lives','v'), lemma.lemmatize('lives','n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WztwUTZ5DujS",
        "outputId": "72d72570-0f4f-4498-c386-d171e3e0ee13"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('live', 'life')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) 어간 추출"
      ],
      "metadata": {
        "id": "jAFD6mDpD1pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\""
      ],
      "metadata": {
        "id": "AcF9N1qoEGpb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출 전\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrID8WwiEhu0",
        "outputId": "11df441f-591c-46d8-b645-4f7e9f346b93"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 어간 추출 후 \n",
        "print([ps.stem(word) for word in word_tokenize(text)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-9U489iEk1d",
        "outputId": "dab4902c-21d8-4d30-d024-cb71536eab8a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['formalize', 'allowance', 'electricical']\n",
        "print([ps.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upPOEIQhEt25",
        "outputId": "42f33eb1-afdc-4e44-8d83-5abca64dbb67"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter Stemmer\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([ps.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijt8qJh9FaBz",
        "outputId": "3cb89788-1651-4983-8071-59c42a731e0b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lancaster Stemmer\n",
        "from nltk.stem import LancasterStemmer \n",
        "ls = LancasterStemmer()\n",
        "print([ls.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hsrrZPQFxGk",
        "outputId": "645b8a6e-7a76-4bfe-c43f-2775568060fa"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fupg7urcGIBd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}