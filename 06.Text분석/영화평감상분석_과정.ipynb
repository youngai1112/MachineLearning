{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 가져오기\n",
    "2. 데이터 전처리\n",
    "    - 트레인 데이터세트와 테스트 데이터세트 준비 (과정 동일)\n",
    "        - 순서 (column1: document=> 감상평, column2: label => 긍정(0) /부정(1) )\n",
    "            - 1. Null 데이터 확인: DF.column1.isna().sum()\n",
    "            - 2. Null 데이터 제거: DF.dropna(how='any', inplace=True) -> shape 확인 (DF.shape - 1.) \n",
    "            - 3. 중복 여부 확인: DF.column1.nunique() => 2.에서 중복된 값을 제외한 실제값이 들어있는 부분 출력\n",
    "            - 4. 중복 데이터 제거: DF.drop_duplicates(subset=['column1'], inplace=True) -> shape 확인 (3.과 동일)\n",
    "            - 5. 데이터 분포 확인(긍정 / 부정): DF.column2.value_counts()\n",
    "3. 텍스트 전처리\n",
    "    - 트레인 데이터세트와 테스트 데이터세트 전처리 (과정 동일)\n",
    "        - 순서 \n",
    "            - 1. 한글 이외의 문자 공백처리하고 strip: DF.column1 = DF.column1.str.replace('[ㄱ-ㅎㅏ-ㅣ가-힣]').str.strip() -> DF.head() 확인\n",
    "                - 주의: 영어로 작성된 감상평이 있다면 정규표현식을 거치면서 공백처리됨: ''로 출력\n",
    "            - 2. '' -> np.nan -> 제거: DF.column1.replace('', np.nan, inplace=True) -> DF.column1.isna().sum()의 결과로 날아가는 데이터의 개수를 확인할 수 있다. \n",
    "            - 3. Null 데이터 제거: DF.dropna(how='any', inplace=True) -> DF.shape로 확인\n",
    "4. 데이터 저장 (train 데이터: DF1, test 데이터: DF2)\n",
    "    - DF1.to_csv('폴더명/파일이름_전처리완료.csv', sep='\\t', index=False)\n",
    "    - DF2.to_csv('폴더명/파일이름_전처리완료.csv', sep='\\t', index=False)\n",
    "    - 데이터 생성되었는지 확인\n",
    "5. 한글 처리 (SOYNLP 사용)\n",
    "    - import joblib; scores = joblib.load('data/scores.pkl')\n",
    "        - joblib으로 불러오는 scores는?\n",
    "            - soynlp\n",
    "                - 학습을 통해 단어 사전을 만듦: import urllib.requests; urllib.request.urlretrieve('링크/파일이름.txt', filename='폴더명/파일이름.txt')\n",
    "                - 훈련 데이터를 문서로 분리: from soynlp import DoublespaceLineCorpus; corpus=DoublespaceLineCorpus('폴더명/파일이름.txt')\n",
    "                - 말뭉치에서 단어 점수표 학습: from soynlp.word import WordExtractor; word_extractor = WordExtractor(); word_extractor.train(corpus); word_score_table=word_extractor.extract()\n",
    "                - 토큰화: from soynlp.tokenizer import Ltokenizer; scores={word:score.cohesion_forward for word, score in word_score_table.items()}; l_tokenizer=LTokenizer(scores=scores); l_tokenizer.tokenize(\"예문\", flatten=False)\n",
    "                - joblib으로 저장: import joblib; joblib.dump(scores, '폴더명/scores.pkl') (2022-01-06, 21.Soynlp 참고)\n",
    "                    - 명사 추출: from soynlp.noun import LRNounExtractor_v2; noun_extractor=NRNounExtractor_v2(verbose=True); nouns=noun_extractor.train_extract(corpus)\n",
    "                        - joblib으로 저장: noun_scores={noun:scores[0] for noun, score in nouns.items() if len(noun) > 1}; joblib.dump(noun_scores, '폴더명/noun_scores.pkl')\n",
    "    - 한글 vector화: 한글 vector화 -> X_train 데이터 (X_train: CountVectorize하기 위해 가공된 형태)\n",
    "        - 1. 형태소로 tokenize: from soynlp.tokenizer import MAxScoreTokenizer; max_tokenizer=MaxScoreTokenizer(scores); text=\"예문\"; word_list=max_tokenizer.tokenize(text)\n",
    "        - 2. stop_text 설정: stop_text='직접 설정한 불용어 쉼표 없이 나열'; stop_list=stop_text.split()\n",
    "        - 3. 불용어를 제거한 형태소 추출: word_list = [word for word in word_list if word not in stop_list]\n",
    "        - 4. 다시 string으로 만들기: review=' '.join(word_list)\n",
    "    - vector화한 문자열을 X_train, X_test, y_train, y_test에 적용하기 \n",
    "        - X_train에 적용: X_train=[]; for document in DF1.column1[:3]: ; word_list=max_tokenizer.tokenize(column1); word_list=[word for word in word_list if word not in stop_list]; tokenized_str=' '.join(word_list); X_train.append(tokenized_str)\n",
    "            - X_train 확인\n",
    "            - DF1.column1[:3] : 원래 모양과 비교\n",
    "        - X_test 적용: X_train과 동일\n",
    "        - y_train: y_train=DF1.column2.values\n",
    "        - y_test: y_test=DF2.column2.values\n",
    "6. Feature 변환: CounterVectorizer\n",
    "    - from sklearn.feature_extraction.text import CountVectorizer; cvect=CountVectorizer(); cvect.fit(X_train); X_train_cv=cvect.transform(X_train); X_test_cv=cvect.transform(X_test) -> X_train_cv.shape, X_test.shape 확인\n",
    "7. 모델 생성 / 학습 / 평가\n",
    "    - 모델 import -> 객체화 -> 객체화된 모델.fit(Feature변환한 X_train, y_train); 객체화된 모델.score(Feature변환한 X_test, y_test)\n",
    "8. 실제 테스트 순서\n",
    "    - 1. review1, review2 문자열로 들고오기\n",
    "    - 2. 전처리\n",
    "        - 주의: DF는 정규표현식을 사용할 수 있지만, 문자열은 정규표현식을 사용할 수 없다\n",
    "            - 해결: import re; review1=re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣]', ' ', review1).strip(); review2=re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣]', ' ', review2).strip()\n",
    "    - 3. 형태소 분석 및 불용어 처리 (review1, review2 과정 동일)\n",
    "        - words=max_tokenizer.tokenize(review1); words=[word for word in words if word not in stop_list]; review1=' '.join(words)\n",
    "    - 4. Feature 변환 (CountVectorizer)\n",
    "        - review=[review1, review2]; review_cv=cvect.transform(review)\n",
    "    - 5. 예측\n",
    "        - 모델.predict(review_cv)\n",
    "9. 튜닝 (모델의 성능향상을 위해 파라미터 조절) : GridSearchCV 사용 => 최적의 파라미터 찾고, 교차검증을 해주는 역할을 한다.\n",
    "    - from sklearn.model_selection import GridSearchCV; from sklearn.pipeline import Pipeline\n",
    "    - pipeline 설정: pipeline=PipeLine([('cvect', CountVectorizer()), ('lrc', LogisticRegression(random_state=2022))]) # (객체화된 클래스명, 모델 클래스)\n",
    "    - 파라미터 설정: params={'cvect__ngram_range': [(1,1),(1,2)], 'cvect__max_df':[0.9, 0.99], 'cvect__min_df': [1,3], 'lrc__C':[1,5]}\n",
    "    - grid_pipe=GridSearchCV(pipeline, param_grid=params, scoring='accuracy', cv=3)\n",
    "    - 최적의 파라미터 확인: grid_pipe.best_params_\n",
    "    - 최적의 측정기: grid_pipe.best_estimator_.score(X_test, y_test)\n",
    "10. 다른 모델을 사용하여 정확도 측정하고 비교해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
